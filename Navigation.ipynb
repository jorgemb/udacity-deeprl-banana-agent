{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use autoreload extension so agent definition is always up to date.\n",
    "# Src: https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Initial imports\n",
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import BananaAgent as Agents\n",
    "\n",
    "sns.set_style()\n",
    "\n",
    "print(f\"Cuda available? {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check if Banana is available\n",
    "# Load environment and get initial brain\n",
    "env = UnityEnvironment(\"Banana/Banana.x86_64\", no_graphics=True)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# Initialize environment for use of the agent\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "action_space = brain.vector_action_space_size\n",
    "state_space = env_info.vector_observations.size\n",
    "\n",
    "print(f\"Action space: {action_space}\")\n",
    "print(f\"State space: {state_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_episode(environment, agent):\n",
    "    \"\"\"Performs a single episode using the given environment and agent\n",
    "\n",
    "    Args:\n",
    "        environment (env): Environment that will perform the simulation\n",
    "        agent (Agent): Agent that will traverse the environment\n",
    "\n",
    "    Returns:\n",
    "        (float, int): Total score and steps of the episode\n",
    "    \"\"\"\n",
    "    episode_score = 0\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "    # Start the agent\n",
    "    state = env_info.vector_observations[0]\n",
    "    next_action = agent.start(state)\n",
    "\n",
    "    # Take the first action\n",
    "    env_info = env.step(next_action)[brain_name]\n",
    "\n",
    "    while not env_info.local_done[0]:\n",
    "        # Take a step from the agent\n",
    "        reward = env_info.rewards[0]\n",
    "        episode_score += reward\n",
    "        state = env_info.vector_observations[0]\n",
    "\n",
    "        next_action = agent.step(reward, state)\n",
    "\n",
    "        # Perform action\n",
    "        env_info = env.step(next_action)[brain_name]\n",
    "    \n",
    "    # Register last reward to the agent\n",
    "    reward = env_info.rewards[0]\n",
    "    episode_score += reward\n",
    "    agent.end(reward)\n",
    "\n",
    "    return episode_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agents():\n",
    "    \"\"\"Create the list of agents to test\n",
    "\n",
    "    Returns:\n",
    "        list: List of agents\n",
    "    \"\"\"\n",
    "    agent_gamma = 0.99\n",
    "    agent_alpha = 5e-4\n",
    "    agent_tau = 1e-3\n",
    "    agent_batch_size = 64\n",
    "    agent_memory_size = int(1e5)\n",
    "    agent_learn_update = 4\n",
    "    seed = 134123\n",
    "\n",
    "    agents = [\n",
    "        # Agents.RandomAgent(state_space, action_space, agent_gamma, agent_alpha, seed),\n",
    "        Agents.Udagent(state_space, action_space, seed=seed),\n",
    "        # Agents.BananaAgent(state_space, action_space,\n",
    "        #                    gamma=agent_gamma,\n",
    "        #                    alpha=agent_alpha,\n",
    "        #                    seed = seed,\n",
    "        #                    tau=agent_tau,\n",
    "        #                    buffer_size=agent_memory_size,\n",
    "        #                    batch_size=agent_batch_size,\n",
    "        #                    learn_every=agent_learn_update),\n",
    "        # Agents.BananaAgentDouble(state_space, action_space,\n",
    "        #                    gamma=agent_gamma,\n",
    "        #                    alpha=agent_alpha,\n",
    "        #                    seed = seed,\n",
    "        #                    tau=agent_tau,\n",
    "        #                    buffer_size=agent_memory_size,\n",
    "        #                    batch_size=agent_batch_size,\n",
    "        #                    learn_every=agent_learn_update)\n",
    "\n",
    "    ]\n",
    "\n",
    "    return agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_experiment(environment, agent, episodes, print_every):\n",
    "    \"\"\"Performs an experiment on the given agent.\n",
    "\n",
    "    Args:\n",
    "        environment (any): Environment to use\n",
    "        agent (Agent): Agent that follows the \"Agent\" interface\n",
    "        episodes (int): Amount of episodes to perform\n",
    "        print_every (int): How often to print the episode information\n",
    "\n",
    "    Returns:\n",
    "        (array_like, array_like): Scores and times that the agent took per episode\n",
    "    \"\"\"\n",
    "    scores = np.zeros(episodes)\n",
    "    times = np.zeros(episodes)\n",
    "\n",
    "    for i in range(episodes):\n",
    "        start_time = time.time()\n",
    "        scores[i] = do_episode(environment, agent)\n",
    "        times[i] = time.time() - start_time\n",
    "\n",
    "        ep = i+1\n",
    "        if ep % print_every == 0:\n",
    "            print(f\"{agent.agent_name()} :: ({ep}/{episodes}) AVG {np.average(scores[max(0, i-print_every):])}\")\n",
    "    \n",
    "    return scores, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_window = 100\n",
    "\n",
    "episodes = 1000\n",
    "print_every = 100\n",
    "\n",
    "# Perform each episode of the training\n",
    "figure, axis = plt.subplots(3, 1, figsize=(19, 9), sharex=\"all\")\n",
    "axis[0].set_title(\"Scores\")\n",
    "axis[0].set_xlabel(\"Steps\")\n",
    "axis[1].set_title(\"Scores (rolling avg)\")\n",
    "axis[1].set_xlabel(\"Steps\")\n",
    "axis[2].set_title(\"Time\")\n",
    "axis[2].set_xlabel(\"Steps\")\n",
    "\n",
    "for agent in create_agents():\n",
    "    # Do experiment\n",
    "    scores, times = do_experiment(env, agent, episodes, print_every)\n",
    "    agent_name = agent.agent_name()\n",
    "\n",
    "    # Save agent\n",
    "    torch.save(agent, f\"agents/{agent_name}.pt\")\n",
    "\n",
    "    # Plot the statistics\n",
    "    x = np.arange(episodes)\n",
    "    sns.lineplot(x=x, y=scores, label=agent_name, ax=axis[0])\n",
    "\n",
    "    avg = [np.average(scores[max(0, n-average_window):n+1]) for n in range(episodes)]\n",
    "    sns.lineplot(x=x, y=avg, label=agent_name, ax=axis[1])\n",
    "    \n",
    "    sns.lineplot(x=x, y=times, label=agent_name, ax=axis[2])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('drlnd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82a110c2bffcf48c88e42faab9efb7876a35677d7810bb5d5a230285c18f640c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
